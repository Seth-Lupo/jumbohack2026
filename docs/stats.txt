JumBuddy Student Analytics — Statistics & AI Report Documentation
=================================================================

This document explains how student coding activity is analyzed, what metrics
are computed, and how AI reports are generated.

================================================================================
1. DATA MODEL: FLUSHES
================================================================================

A "flush" is a window of editing activity captured from the VS Code extension:
  - file_path: Which file was edited
  - active_symbol: Function/class the cursor was in
  - diffs: Unified diff showing changes
  - start_timestamp / end_timestamp: Time range
  - window_duration: How long the edit window lasted (seconds)
  - sequence_number: Ordering guarantee for reconstruction

Flushes are the raw data. All metrics derive from analyzing flush sequences.

================================================================================
2. ALGORITHMIC METRICS (No LLM — Pure Computation)
================================================================================

2.1 EDIT REGIONS
----------------
Each flush becomes ONE EditRegion (hunks aggregated to prevent time overcounting):
  - line_start, line_end: Range of all lines touched (min/max across hunks)
  - chars_inserted, chars_deleted: Total changes across all hunks
  - net_change: inserted - deleted
  - duration_sec: flush.window_duration (counted ONCE per flush)

CRITICAL: One region per flush, NOT one per hunk.
This prevents overcounting time when a flush has multiple diff hunks.

2.2 LINGER SCORE (Algorithm 1: Struggle Detection)
---------------------------------------------------
For each (file_path, symbol) pair across all flushes:

  dwell_time = sum of window_duration for all flushes touching this symbol
  visits = number of distinct flushes (each flush = one visit)
  churn = (chars_inserted + chars_deleted) / |net_change|

  High churn means lots of rewriting (e.g., 200 chars changed but only 50 net).

  dwell_norm = dwell_time / max_dwell_across_all_symbols
  linger_score = dwell_norm × churn × visits

Where students struggle:
  - High linger score = lots of time + high churn + many revisits
  - Ranked descending by linger_score

2.3 CURRENT FOCUS (Algorithm 2: Recency-Weighted Activity)
-----------------------------------------------------------
What is the student working on *right now*?

  For each symbol:
    age_minutes = (latest_timestamp - symbol_end_time) / 60
    decay = exp(-age_minutes / 30)  // 30-min decay window
    weighted_time = sum(duration × decay)

  Ranked descending by weighted_time.
  Recent work weighs more; old work decays exponentially.

2.4 CLASS-WIDE STRUGGLE (Algorithm 3: Aggregated Struggles)
------------------------------------------------------------
Which symbols/functions cause the most students to struggle?

  1. Compute linger scores per student
  2. Normalize symbol names (lowercase, trim)
  3. For each symbol across all students:
       student_count = number of students with linger_score > threshold
       avg_linger = mean linger score among struggling students
       struggle_index = student_count × avg_linger

  Ranked descending by struggle_index.
  Shows class-wide pain points.

2.5 DISPLAYED STATS
-------------------
Student Stats:
  - Time on Assignment: ACTIVE TIME ONLY
    Filters: window_duration < 5 minutes AND has diffs
    Excludes: idle periods, long pauses, empty flushes
    Formula: sum(flush.window_duration for flush in active_flushes)
  - Top Struggle Area: symbol with highest linger_score
  - Current Focus: symbol with highest recency-weighted time
  - Avg Churn Rate: mean churn across all symbols

Class Stats:
  - Students Tracked: distinct profile_id count
  - Total Flushes: total flush count
  - Top Struggle Symbol: symbol with highest struggle_index
  - Struggling Students: student_count for top struggle symbol

Linger Breakdown Table (per student):
  - Symbol, Time (dwell_time), Visits, Churn
  - Top 5 by linger_score

================================================================================
3. AI REPORT GENERATION (LLM-Based Semantic Analysis)
================================================================================

3.1 PREPROCESSING: TIMELINE CONSTRUCTION (WHITESPACE PRESERVED)
----------------------------------------------------------------
Raw flushes are transformed into a human-readable timeline with exact
whitespace preservation for accurate code analysis:

  1. Sort flushes by timestamp
  2. Group into sessions (gap > 30 min = new session)
  3. For each session:
       - Show time range and duration
       - Group flushes by (file_path, symbol)
       - For each group:
           * Count visits
           * Sum time spent
           * Parse diffs to count +lines/-lines
           * Extract sample added/removed snippets
           * Flag high churn (total_changed / |net| > 3)
           * Flag many revisits (visits > 3)

Output format:
  === Session 1 ===
  Time: 2024-01-15 10:30 to 11:15 (12m 30s)
  Files worked on: 2

    main.py → calculate():
      Time: 450s, Visits: 4
      Changes: +23 lines, -18 lines
      ⚠ High churn (rewrote multiple times)
      ⚠ Revisited 4 times (possible struggle)
      Sample changes:
        added: def calculate(x, y):
        removed: return x + y

This gives the LLM semantic understanding of what the student actually did.

WHITESPACE HANDLING:
  - Indentation is PRESERVED when extracting diff snippets
  - Leading/trailing spaces kept intact (critical for Python, YAML, etc.)
  - Tab characters preserved (not converted to spaces)
  - Empty lines skipped, but indented lines with code kept
  - Max 100 chars per line (truncate long lines, not indentation)
  - Full diff hunks shown for significant changes (with @@ context)

This ensures the LLM sees proper code structure and can understand:
  - Indentation-based scoping (Python, YAML)
  - Code blocks and nesting levels
  - Formatting changes vs. logic changes

3.2 LLM PROMPT CONSTRUCTION
----------------------------
Prompt includes:
  - The full preprocessed timeline (session-by-session breakdown)
  - Algorithmic struggle summary (top 5 linger scores)
  - Total time spent
  - Directive to analyze:
      * What did they work on? (specific files/functions)
      * Where did they struggle? (high churn, revisits, time)
      * Were they productive or spinning wheels?
      * What should the TA focus on to help?

Model: gpt-4o-mini
Max tokens: 500
Temperature: 0.3 (low for consistency)

System prompt:
  "You are an experienced teaching assistant analyzing student code
   development patterns. Focus on semantic understanding of their
   workflow and struggles."

3.3 OUTPUT
----------
The LLM returns a 4-6 sentence actionable report:
  - Specific about what files/functions the student worked on
  - Identifies concrete struggle points (not just "they struggled")
  - Assesses productivity (actual progress vs. wheel-spinning)
  - Gives TA actionable advice

Example:
  "The student spent 18 minutes primarily working on the calculate()
   function in main.py, making 4 separate editing passes with high
   churn (23 additions, 18 deletions). They appear to have struggled
   with implementing error handling, repeatedly rewriting the same
   section. The validate_input() function was also revisited 3 times
   but with lower churn, suggesting minor refinements. The TA should
   review their error handling approach in calculate() as this is
   where most time was lost."

================================================================================
4. HEATMAP VISUALIZATION
================================================================================

Heatmap rows: Students (by UTLN)
Heatmap columns: Top 6 struggle symbols (from class analysis)
Cell values: That student's linger_score for that symbol (scaled 0-100)

Color mapping:
  - 0-50: Green → Yellow (HSL hue 142 → 45)
  - 50-100: Yellow → Red (HSL hue 45 → 0)
  - Saturation/lightness increase with struggle

Click a cell to see:
  "X visited function Y N times over Tm Ts with churn rate C"

================================================================================
5. KEY DESIGN PRINCIPLES
================================================================================

✓ Algorithm-first: All metrics are explainable, deterministic computations
✓ LLM as presentation layer: AI translates metrics + diffs into prose
✓ Semantic understanding: LLM sees actual code changes, not just numbers
✓ Traceable: Every stat can be drilled down to source flushes/diffs
✓ No black boxes: TAs can verify AI claims against raw timeline data

================================================================================
6. WHITESPACE & RECONSTRUCTION GUARANTEES
================================================================================

6.1 DIFF CREATION (VS Code Extension)
--------------------------------------
When the extension creates diffs (`differ.ts`):

  ✓ Line endings normalized to LF (\n)
    - Prevents CRLF (Windows) vs LF (Unix/Mac) mismatches
    - Ensures cross-platform compatibility

  ✓ Whitespace preserved exactly:
    - Tabs remain tabs (not converted to spaces)
    - Spaces remain spaces
    - Leading indentation preserved
    - Trailing whitespace preserved (no auto-trim)

  ✓ Options: ignoreWhitespace: false
    - Forces `diff` library to treat whitespace as significant
    - Every space/tab/newline matters

6.2 RECONSTRUCTION (Web Replay)
--------------------------------
When reconstructing files during replay (`reconstruct.ts`):

  ✓ Line ending normalization:
    - Both content and diff normalized to LF before applying patch
    - Prevents platform-specific line ending issues

  ✓ Whitespace fidelity:
    - applyPatch preserves exact whitespace from diff
    - No trimming, no expansion, no normalization (except LF)

  ✓ Error handling:
    - If patch fails, logs detailed error to console
    - Returns last known good state (partial reconstruction)
    - Allows debugging of whitespace-related patch failures

6.3 CONTENT HASH VERIFICATION
------------------------------
Each flush has a SHA-256 content_hash of the file AFTER applying the diff.

To verify reconstruction correctness:
  1. Reconstruct file at step N
  2. Compute SHA-256 hash of reconstructed content
  3. Compare with flush[N].content_hash
  4. If mismatch → whitespace drift or patch failure

This guarantees bit-perfect reconstruction when hashes match.

6.4 COMMON WHITESPACE PITFALLS (AVOIDED)
-----------------------------------------
❌ WRONG: Tabs converted to spaces → breaks Python, Makefiles
   ✓ FIXED: Tabs preserved as-is

❌ WRONG: CRLF/LF mismatch → patch fails on Windows files
   ✓ FIXED: All normalized to LF at diff creation & reconstruction

❌ WRONG: Trailing whitespace trimmed → breaks string literals, heredocs
   ✓ FIXED: No auto-trimming anywhere in pipeline

❌ WRONG: ignoreWhitespace: true → loses indentation changes
   ✓ FIXED: ignoreWhitespace: false (whitespace is significant)

6.5 TESTING WHITESPACE CORRECTNESS
-----------------------------------
To verify whitespace handling:

  1. Create a file with mixed tabs/spaces, trailing whitespace
  2. Make edits in VS Code extension
  3. Check replay reconstruction matches exactly
  4. Verify content_hash matches at each step
  5. Check console for any patch failure errors

If reconstruction fails:
  - Check console.error logs for patch failure details
  - Verify line endings are consistent (LF)
  - Check for non-printable characters (BOM, etc.)

================================================================================
7. SYNTAX HIGHLIGHTING (REPLAY)
================================================================================

The replay code viewer includes syntax highlighting for common languages.

Supported Languages:
  ✓ C / C++        (.c, .h, .cpp, .hpp, .cc, .cxx, .hh)
  ✓ Python         (.py)
  ✓ Java           (.java)
  ✓ JavaScript     (.js)
  ✓ TypeScript     (.ts)
  ✓ JSX / TSX      (.jsx, .tsx)
  ✓ Rust           (.rs)
  ✓ Go             (.go)
  ✓ Ruby           (.rb)
  ✓ Bash           (.sh, .bash)
  ✓ SQL            (.sql)
  ✓ JSON           (.json)
  ✓ YAML           (.yaml, .yml)
  ✓ HTML           (.html, .htm, .xml)
  ✓ CSS            (.css)

Color Scheme: VS Code Dark+ (dark background)

Token Colors:
  - Keywords (if, for, class):     Purple (#C586C0)
  - Functions:                      Yellow (#DCDCAA)
  - Strings:                        Orange (#CE9178)
  - Numbers:                        Light Green (#B5CEA8)
  - Comments:                       Green (#6A9955)
  - Variables/Properties:           Light Blue (#9CDCFE)
  - Types/Classes:                  Teal (#4EC9B0)
  - Operators:                      White (#D4D4D4)

Highlighting Features:
  ✓ Works with typing animation (character-by-character)
  ✓ Preserves cursor position highlighting (green/red)
  ✓ Preserves edit region highlighting
  ✓ Preserves whitespace (spaces, tabs, indentation)
  ✓ Auto-detects language from file extension
  ✓ Graceful fallback for unknown languages (plain text)

Implementation:
  - Uses Prism.js for tokenization
  - Tokens applied per-character for animation compatibility
  - Zero performance impact (tokenization happens once per step)

================================================================================
8. EXTENSION DEBUG MODE
================================================================================

To enable detailed flush logging in VS Code:

1. Edit .jumbuddy/config.json:
   {
     "utln": "your_utln",
     "key": "your_key",
     "assignment_id": "...",
     "course_id": "...",
     "server_url": "...",
     "debug": true  // ADD THIS LINE
   }

2. Reload VS Code extension (or restart VS Code)

3. Open "View" → "Output" → Select "JumBuddy Debug" from dropdown

4. As you edit files, detailed flush stats will appear:
   - File path, sequence number, trigger
   - Active symbol, time window, duration
   - Character changes (inserted/deleted)
   - Lines touched, edit velocity, rewrite ratio
   - Thrash score, error churn, delete/rewrite stats
   - Content hash (for verification)
   - Diff preview (first 10 lines)
   - Push status (success/failure)

This helps verify:
  ✓ Flushes are being created correctly
  ✓ Window durations are accurate
  ✓ Diffs contain expected changes
  ✓ Metrics are computed correctly
  ✓ Pushes are succeeding

To disable: Set "debug": false (or remove the line)

================================================================================
8. DEBUGGING & VERIFICATION
================================================================================

Endpoints:
  GET /api/analysis/student/<id>?assignment_id=X
    → Returns raw linger/focus metrics (no LLM)

  GET /api/analysis/report/<id>?assignment_id=X
    → Generates AI report (with LLM)

  GET /api/analysis/class/<assignment_id>
    → Class-wide struggle topics

  GET /api/analysis/secret
    → Debug: Check if OPENAI_API_KEY is loaded

To verify a linger score:
  1. Check student's flushes for that symbol
  2. Sum window_duration (dwell_time)
  3. Count distinct flushes (visits)
  4. Parse diffs to compute churn
  5. Apply formula: (dwell_norm × churn × visits)

To verify AI report accuracy:
  1. Read the preprocessed timeline in server logs (if logged)
  2. Check if AI claims match actual diff content
  3. Verify time/visit counts against raw metrics
